{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generative Models Part 1: An Introduction to GANs\n",
        "This notebook trains **two** generative models on MNIST and saves a *generated* grid (16 x 8), it is meant to reproduce what you see in the article :\n",
        "\n",
        "- **Vanilla GAN** (logistic discriminator, non-saturating generator loss)\n",
        "- **Vanilla SGD** (logistic discriminator, non-saturating generator loss)\n",
        "- **WGAN-GP** (critic + gradient penalty)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports & Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "import torch.utils.data as data\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device is', device)\n",
        "\n",
        "# Displaying function \n",
        "def imshow(img, size=None):\n",
        "    img = img*0.5 + 0.5  # unnormalize\n",
        "    if size is not None:\n",
        "        img = transforms.Resize(size=size, interpolation=transforms.InterpolationMode.NEAREST, antialias=True)(img)\n",
        "    pil_img = torchvision.transforms.functional.to_pil_image(img)\n",
        "    display(pil_img)\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Fetch the DATA (MNIST normalized to [-1, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_set = MNIST(os.getcwd(), train=True, transform=transform, download=True)\n",
        "train_loader = data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
        "\n",
        "# Visual sanity check\n",
        "real, _ = next(iter(train_loader))\n",
        "print(real.shape)\n",
        "imshow(torchvision.utils.make_grid(real.to('cpu'), nrow=16))\n",
        "print(\"Sample of the data that we want to generate\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Class of the Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Size of generator input\n",
        "nz = 100\n",
        "\n",
        "# Size of feature maps in generator and discriminator\n",
        "ngf, ndf = 64, 64\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=nz, out_channels=ngf * 8, kernel_size=4, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(in_channels=ngf * 8, out_channels=ngf * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d(in_channels=ngf * 4, out_channels=ngf * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d(in_channels=ngf * 2, out_channels=ngf, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf) x 32 x 32\n",
        "            nn.ConvTranspose2d(in_channels=ngf, out_channels=1, kernel_size=1, stride=1, padding=2, bias=False),\n",
        "            nn.Tanh()\n",
        "            # output size. 1 x 28 x 28\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # input is 1 x 28 x 28\n",
        "            nn.Conv2d(in_channels=1, out_channels=ndf, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf) x 15 x 15\n",
        "            nn.Conv2d(in_channels=ndf, out_channels=ndf * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*2) x 8 x 8\n",
        "            nn.Conv2d(in_channels=ndf * 2, out_channels=ndf * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*4) x 5 x 5\n",
        "            nn.Conv2d(in_channels=ndf * 4, out_channels=1, kernel_size=4, stride=2, padding=1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input).view(-1, 1).squeeze(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Utilities "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function to display samples of the generator (reused)\n",
        "def show(G, z=None, batch_size=128, nz=100):\n",
        "    with torch.no_grad():\n",
        "        if z is None:\n",
        "            z = torch.randn(batch_size, nz, 1, 1).to(device)\n",
        "        genimages = G(z)\n",
        "        imshow(torchvision.utils.make_grid(genimages.to('cpu'), nrow=16))\n",
        "        return None\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vanilla GAN Training (Adam)\n",
        "\n",
        "DCGAN-style architecture unchanged. More specifically, we use the **vanilla GAN** objective with the **non-saturating generator loss**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(1)\n",
        "\n",
        "num_epochs = 20        # set to 100 to match the article snapshots\n",
        "log_every = 200        # batches\n",
        "lr = 2e-4\n",
        "\n",
        "G_adam = Generator().to(device)\n",
        "D_adam = Discriminator().to(device)\n",
        "G_adam.apply(weights_init);\n",
        "D_adam.apply(weights_init);\n",
        "\n",
        "optimD = optim.Adam(D_adam.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "optimG = optim.Adam(G_adam.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "fixed_z = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "\n",
        "D_losses_adam, G_losses_adam = [], []\n",
        "\n",
        "t0 = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (real, _) in enumerate(train_loader):\n",
        "        real = real.to(device)\n",
        "\n",
        "        # --------------------\n",
        "        # (1) Update Discriminator: maximize log D(x) + log(1 - D(G(z)))\n",
        "        # --------------------\n",
        "        optimD.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Real\n",
        "        logits_real = D_adam(real)\n",
        "        labels_real = torch.ones_like(logits_real, device=device)\n",
        "        lossD_real = criterion(logits_real, labels_real)\n",
        "\n",
        "        # Fake\n",
        "        z = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "        fake = G_adam(z)\n",
        "        logits_fake = D_adam(fake.detach())\n",
        "        labels_fake = torch.zeros_like(logits_fake, device=device)\n",
        "        lossD_fake = criterion(logits_fake, labels_fake)\n",
        "\n",
        "        lossD = lossD_real + lossD_fake\n",
        "        lossD.backward()\n",
        "        optimD.step()\n",
        "\n",
        "        # --------------------\n",
        "        # (2) Update Generator: maximize log D(G(z)) (non-saturating)\n",
        "        # --------------------\n",
        "        optimG.zero_grad(set_to_none=True)\n",
        "        z2 = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "        fake2 = G_adam(z2)\n",
        "        logits_fake2 = D_adam(fake2)\n",
        "        labels_gen = torch.ones_like(logits_fake2, device=device)\n",
        "        lossG = criterion(logits_fake2, labels_gen)\n",
        "        lossG.backward()\n",
        "        optimG.step()\n",
        "\n",
        "        # Logging\n",
        "        if i % log_every == 0:\n",
        "            D_losses_adam.append(lossD.item())\n",
        "            G_losses_adam.append(lossG.item())\n",
        "            print(f\"[Adam] epoch {epoch:03d} | iter {i:04d} | lossD {lossD.item():.3f} | lossG {lossG.item():.3f} | t={(time.time()-t0)/60:.1f}m\")\n",
        "\n",
        "    # End of epoch snapshot\n",
        "    print(f\"[Adam] Snapshot at epoch {epoch+1}\")\n",
        "    G_adam.eval()\n",
        "    show(G_adam, z=fixed_z, batch_size=batch_size, nz=nz)\n",
        "    G_adam.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loss curves (Adam)\n",
        "plt.figure()\n",
        "plt.plot(D_losses_adam, label='D (Adam)')\n",
        "plt.plot(G_losses_adam, label='G (Adam)')\n",
        "plt.legend()\n",
        "plt.title('Vanilla GAN (Adam) — losses (logged every few batches)')\n",
        "plt.xlabel('log step')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Vanilla GAN Training (SGD)\n",
        "\n",
        "Same objective as above, but we replace Adam by **plain SGD**. On MNIST with this setup, training is typically much less stable and can exhibit **mode collapse**, as discussed in the article."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(1)\n",
        "\n",
        "num_epochs = 20        # set to 100 to reproduce longer-run collapse\n",
        "log_every = 200\n",
        "\n",
        "# SGD hyperparameters: feel free to tune to observe collapse more clearly\n",
        "lr_sgd = 2e-4\n",
        "momentum = 0.0 # changing momentum to 0.9 can help show mode collapse\n",
        "\n",
        "# if you aren't getting any luck, try lowering the batch size (to 16 for example)\n",
        "\n",
        "G_sgd = Generator().to(device)\n",
        "D_sgd = Discriminator().to(device)\n",
        "G_sgd.apply(weights_init);\n",
        "D_sgd.apply(weights_init);\n",
        "\n",
        "optimD = optim.SGD(D_sgd.parameters(), lr=lr_sgd, momentum=momentum)\n",
        "optimG = optim.SGD(G_sgd.parameters(), lr=lr_sgd, momentum=momentum)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "fixed_z = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "\n",
        "D_losses_sgd, G_losses_sgd = [], []\n",
        "\n",
        "t0 = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (real, _) in enumerate(train_loader):\n",
        "        real = real.to(device)\n",
        "\n",
        "        # (1) Discriminator\n",
        "        optimD.zero_grad(set_to_none=True)\n",
        "\n",
        "        logits_real = D_sgd(real)\n",
        "        labels_real = torch.ones_like(logits_real, device=device)\n",
        "        lossD_real = criterion(logits_real, labels_real)\n",
        "\n",
        "        z = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "        fake = G_sgd(z)\n",
        "        logits_fake = D_sgd(fake.detach())\n",
        "        labels_fake = torch.zeros_like(logits_fake, device=device)\n",
        "        lossD_fake = criterion(logits_fake, labels_fake)\n",
        "\n",
        "        lossD = lossD_real + lossD_fake\n",
        "        lossD.backward()\n",
        "        optimD.step()\n",
        "\n",
        "        # (2) Generator (non-saturating)\n",
        "        optimG.zero_grad(set_to_none=True)\n",
        "        z2 = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "        fake2 = G_sgd(z2)\n",
        "        logits_fake2 = D_sgd(fake2)\n",
        "        labels_gen = torch.ones_like(logits_fake2, device=device)\n",
        "        lossG = criterion(logits_fake2, labels_gen)\n",
        "        lossG.backward()\n",
        "        optimG.step()\n",
        "\n",
        "        if i % log_every == 0:\n",
        "            D_losses_sgd.append(lossD.item())\n",
        "            G_losses_sgd.append(lossG.item())\n",
        "            print(f\"[SGD] epoch {epoch:03d} | iter {i:04d} | lossD {lossD.item():.3f} | lossG {lossG.item():.3f} | t={(time.time()-t0)/60:.1f}m\")\n",
        "\n",
        "    print(f\"[SGD] Snapshot at epoch {epoch+1}\")\n",
        "    G_sgd.eval()\n",
        "    show(G_sgd, z=fixed_z, batch_size=batch_size, nz=nz)\n",
        "    G_sgd.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loss curves (SGD)\n",
        "plt.figure()\n",
        "plt.plot(D_losses_sgd, label='D (SGD)')\n",
        "plt.plot(G_losses_sgd, label='G (SGD)')\n",
        "plt.legend()\n",
        "plt.title('Vanilla GAN (SGD) — losses (logged every few batches)')\n",
        "plt.xlabel('log step')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## WGAN-GP\n",
        "\n",
        "We now switch to the WGAN objective (critic outputs real values) and enforce the 1-Lipschitz constraint using the **gradient penalty**:\n",
        "\n",
        "$GP = \\mathbb{E}[(\\|\n",
        "\\nabla_{\\hat x} D(\\hat x)\\|_2 - 1)^2] $\n",
        "\n",
        "We reuse the same `Discriminator` class, but interpret it as a **critic**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gradient penalty \n",
        "def gradient_penalty(D, x_real, x_fake):\n",
        "    alpha = torch.rand((x_real.shape[0], 1, 1, 1), device=device)\n",
        "    x_hat = alpha * x_real + (1 - alpha) * x_fake\n",
        "    x_hat.requires_grad_(True)\n",
        "\n",
        "    d_hat = D(x_hat)\n",
        "    grad_outputs = torch.ones_like(d_hat, device=device)\n",
        "\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=d_hat,\n",
        "        inputs=x_hat,\n",
        "        grad_outputs=grad_outputs,\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True,\n",
        "    )[0]\n",
        "\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    grad_norm = gradients.norm(2, dim=1)\n",
        "    gp = ((grad_norm - 1) ** 2).mean()\n",
        "    return gp\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb626bcf",
      "metadata": {},
      "source": [
        "## WGAN-GP Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(1)\n",
        "\n",
        "num_epochs = 20       # set to 100 for long training\n",
        "log_every = 200\n",
        "\n",
        "# WGAN-GP typical settings\n",
        "lr = 2e-4\n",
        "betas = (0.5, 0.999)\n",
        "n_critic = 5\n",
        "lambda_gp = 10.0\n",
        "\n",
        "G_wgan = Generator().to(device)\n",
        "D_wgan = Discriminator().to(device)\n",
        "G_wgan.apply(weights_init);\n",
        "D_wgan.apply(weights_init);\n",
        "\n",
        "optimD = optim.Adam(D_wgan.parameters(), lr=lr, betas=betas)\n",
        "optimG = optim.Adam(G_wgan.parameters(), lr=lr, betas=betas)\n",
        "\n",
        "fixed_z = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "\n",
        "D_losses_wgan, G_losses_wgan = [], []\n",
        "\n",
        "t0 = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (real, _) in enumerate(train_loader):\n",
        "        real = real.to(device)\n",
        "\n",
        "        # --------------------\n",
        "        # (1) Critic updates\n",
        "        # --------------------\n",
        "        for _ in range(n_critic):\n",
        "            optimD.zero_grad(set_to_none=True)\n",
        "\n",
        "            z = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "            fake = G_wgan(z).detach()\n",
        "\n",
        "            d_real = D_wgan(real).mean()\n",
        "            d_fake = D_wgan(fake).mean()\n",
        "            gp = gradient_penalty(D_wgan, real, fake)\n",
        "\n",
        "            # We *minimize* the negative of the WGAN objective\n",
        "            lossD = -(d_real - d_fake) + lambda_gp * gp\n",
        "            lossD.backward()\n",
        "            optimD.step()\n",
        "\n",
        "        # --------------------\n",
        "        # (2) Generator update\n",
        "        # --------------------\n",
        "        optimG.zero_grad(set_to_none=True)\n",
        "        z2 = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "        fake2 = G_wgan(z2)\n",
        "        lossG = -D_wgan(fake2).mean()\n",
        "        lossG.backward()\n",
        "        optimG.step()\n",
        "\n",
        "        if i % log_every == 0:\n",
        "            D_losses_wgan.append(lossD.item())\n",
        "            G_losses_wgan.append(lossG.item())\n",
        "            print(f\"[WGAN-GP] epoch {epoch:03d} | iter {i:04d} | lossD {lossD.item():.3f} | lossG {lossG.item():.3f} | t={(time.time()-t0)/60:.1f}m\")\n",
        "\n",
        "    print(f\"[WGAN-GP] Snapshot at epoch {epoch+1}\")\n",
        "    G_wgan.eval()\n",
        "    show(G_wgan, z=fixed_z, batch_size=batch_size, nz=nz)\n",
        "    G_wgan.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loss curves (WGAN-GP)\n",
        "plt.figure()\n",
        "plt.plot(D_losses_wgan, label='Critic (WGAN-GP)')\n",
        "plt.plot(G_losses_wgan, label='G (WGAN-GP)')\n",
        "plt.legend()\n",
        "plt.title('WGAN-GP — losses (logged every few batches)')\n",
        "plt.xlabel('log step')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick qualitative comparison\n",
        "\n",
        "We sample 128 images from each trained generator (same fixed noise), display them as grids and compare the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Put generators in eval mode for clean BatchNorm behavior\n",
        "G_adam.eval(); G_sgd.eval(); G_wgan.eval();\n",
        "\n",
        "with torch.no_grad():\n",
        "    z = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "    xa = G_adam(z).cpu()\n",
        "    xs = G_sgd(z).cpu()\n",
        "    xw = G_wgan(z).cpu()\n",
        "\n",
        "# Build grids\n",
        "grid_a = torchvision.utils.make_grid(xa, nrow=16, normalize=False)\n",
        "grid_s = torchvision.utils.make_grid(xs, nrow=16, normalize=False)\n",
        "grid_w = torchvision.utils.make_grid(xw, nrow=16, normalize=False)\n",
        "\n",
        "# Display sequentially \n",
        "print('Vanilla GAN (Adam)')\n",
        "imshow(grid_a)\n",
        "\n",
        "print('Vanilla GAN (SGD)')\n",
        "imshow(grid_s)\n",
        "\n",
        "print('WGAN-GP')\n",
        "imshow(grid_w)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
